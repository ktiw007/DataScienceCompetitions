# -*- coding: utf-8 -*-
"""Amazon ML Challenge Final Submission (By Kshitij Tiwari).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1cp8NLZb9mM9gNR8nL0eZqhIDmUfWOqQI

#This is the Jupyter notebook for the Amazon ML Engineer Hiring Challenge. Given below is the methodology followed for the whole process-
- Load the dataset
Perform EDA
- Perform data cleaning and impute missing values
- Use different algorithms such as- Logistic Regression, Decision Tree, Random Forest, XGBoost, Support Vector Machine (Classifier)
- Perform Hyper parameter tuning for all relevant models
- Select the best model and deploy

#Adding all the relevant libraries
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import cufflinks as cf
cf.go_offline()
import plotly.io as pio
pio.renderers.default = 'colab'
from sklearn import preprocessing
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
import xgboost as xgb
from xgboost import XGBClassifier
from sklearn import metrics
from sklearn.metrics import *
from hyperopt import STATUS_OK, Trials, fmin, hp, tpe
from sklearn.model_selection import RandomizedSearchCV
from sklearn.model_selection import KFold
from imblearn.over_sampling import SMOTE
from sklearn.model_selection import RepeatedStratifiedKFold
from sklearn.model_selection import GridSearchCV
from sklearn.svm import SVC
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix
from sklearn.model_selection import cross_val_score
# %matplotlib inline

"""###Reading the training and the testing dataset"""

train_df=pd.read_csv('/content/drive/MyDrive/Amazon ML Challenge/train.csv')
test_df=pd.read_csv('/content/drive/MyDrive/Amazon ML Challenge/test.csv')

train_df.head()

test_df.head()

print(train_df.shape,test_df.shape)

"""#Exploratory Data Analysis (EDA)

###Printing the count of missing values in each column
"""

print('For Training Datatset','\n')
print(train_df.isnull().sum(),'\n')
print('For Testing Datatset','\n')
print(test_df.isnull().sum())

"""###Printing the count of each category in the customer_category column"""

print(train_df.customer_category.value_counts())

"""###Dropping customer_id column from both training and testing data set (testing data id saved for further use)"""

train_df.drop(columns=['customer_id'],inplace=True)
test_df_customer_id=test_df['customer_id']
test_df.drop(columns=['customer_id'],inplace=True)

"""In categorical type columns- 'customer_active_segment' and 'X1', replacing null values as a new category as it will help in calculating missing values for other columns"""

train_df['X1']=train_df['X1'].fillna('None')
train_df['customer_active_segment']=train_df['customer_active_segment'].fillna('None')
test_df['customer_active_segment']=test_df['customer_active_segment'].fillna('None')
test_df['X1']=test_df['X1'].fillna('None')

"""###Statistical summary of training anf testing datatset"""

train_df.describe()

test_df.describe()

"""###Correlation Heatmap"""

plt.figure(figsize=(16, 6))
heatmap = sns.heatmap(train_df.corr(), vmin=-1, vmax=1, annot=True)

"""###Histogram plot for each of the columns having missing values"""

train_df['customer_product_search_score'].iplot(kind='hist',bins=50,color='blue',title="customer_product_search_score")
train_df['customer_stay_score'].iplot(kind='hist',bins=50,color='red',title="customer_stay_score")
train_df['customer_product_variation_score'].iplot(kind='hist',bins=50,color='orange',title="customer_product_variation_score")
train_df['customer_order_score'].iplot(kind='hist',bins=50,color='yellow',title="customer_order_score")
train_df['customer_active_segment'].iplot(kind='hist',color='violet',title="customer_active_segment")
train_df['X1'].iplot(kind='hist',color='green',title="X1")

"""###Boxplot of each of the continuous type features as y axis and categorical columns as x axis"""

plt.figure(figsize=(12, 7))
sns.boxplot(x='customer_active_segment',y='customer_product_search_score',data=train_df,palette='Set2' )
plt.figure(figsize=(12, 8))
sns.boxplot(x='customer_active_segment',y='customer_stay_score',data=train_df,palette='Set2' )
plt.figure(figsize=(12, 8))
sns.boxplot(x='customer_active_segment',y='customer_product_variation_score',data=train_df,palette='Set2' )
plt.figure(figsize=(12, 8))
sns.boxplot(x='customer_active_segment',y='customer_order_score',data=train_df,palette='Set2' )

plt.figure(figsize=(12, 7))
sns.boxplot(x='X1',y='customer_product_search_score',data=train_df,palette='Set2' )
plt.figure(figsize=(12, 8))
sns.boxplot(x='X1',y='customer_stay_score',data=train_df,palette='Set2' )
plt.figure(figsize=(12, 8))
sns.boxplot(x='X1',y='customer_product_variation_score',data=train_df,palette='Set2' )
plt.figure(figsize=(12, 8))
sns.boxplot(x='X1',y='customer_order_score',data=train_df,palette='Set2' )

"""###Individual Boxplot of each of the continuous type columns"""

plt.figure(figsize=(12, 7))
sns.boxplot(x='customer_product_search_score',data=train_df,palette='Set2' )
plt.figure(figsize=(12, 8))
sns.boxplot(x='customer_stay_score',data=train_df,palette='Set2' )
plt.figure(figsize=(12, 8))
sns.boxplot(x='customer_product_variation_score',data=train_df,palette='Set2' )
plt.figure(figsize=(12, 8))
sns.boxplot(x='customer_order_score',data=train_df,palette='Set2')

"""###Mean and median calculation for each of the continuous type variable by grouping it with repect to categorical columns"""

print(train_df.groupby('customer_active_segment', as_index=False)['customer_product_search_score'].mean())
print(train_df.groupby('customer_active_segment', as_index=False)['customer_stay_score'].mean())
print(train_df.groupby('customer_active_segment', as_index=False)['customer_product_variation_score'].mean())
print(train_df.groupby('customer_active_segment', as_index=False)['customer_order_score'].mean())

print(train_df.groupby('X1', as_index=False)['customer_product_search_score'].mean())
print(train_df.groupby('X1', as_index=False)['customer_stay_score'].mean())
print(train_df.groupby('X1', as_index=False)['customer_product_variation_score'].mean())
print(train_df.groupby('X1', as_index=False)['customer_order_score'].mean())

print(train_df.groupby('customer_active_segment', as_index=False)['customer_product_search_score'].median())
print(train_df.groupby('customer_active_segment', as_index=False)['customer_stay_score'].median())
print(train_df.groupby('customer_active_segment', as_index=False)['customer_product_variation_score'].median())
print(train_df.groupby('customer_active_segment', as_index=False)['customer_order_score'].median())

print(train_df.groupby('X1', as_index=False)['customer_product_search_score'].median())
print(train_df.groupby('X1', as_index=False)['customer_stay_score'].median())
print(train_df.groupby('X1', as_index=False)['customer_product_variation_score'].median())
print(train_df.groupby('X1', as_index=False)['customer_order_score'].median())

"""###Function for missing value imputation"""

def impute_missing_value(cols,mean_A,mean_AA,mean_B,mean_C,mean_D,mean_None):
    missingValCol = cols[0]
    customer_active_segment = cols[1]
    if pd.isnull(missingValCol):
        if customer_active_segment == 'A':
            return mean_A
        elif customer_active_segment == 'AA':
            return mean_AA
        elif customer_active_segment == 'B':
            return mean_B
        elif customer_active_segment == 'C':
            return mean_C
        elif customer_active_segment == 'D':
            return mean_D
        elif customer_active_segment == 'None':
            return mean_None
    else:
        return missingValCol

"""###First groupwise mean and median values has been checked for each of the continuous type features having missing values. Then the imputation has been done by using groupwise mean for the feature 'customer_active segment' as it had the least variation"""

mean_A=train_df[train_df['customer_active_segment']=='A'].customer_product_search_score.mean()
mean_AA=train_df[train_df['customer_active_segment']=='AA'].customer_product_search_score.mean()
mean_B=train_df[train_df['customer_active_segment']=='B'].customer_product_search_score.mean()
mean_C=train_df[train_df['customer_active_segment']=='C'].customer_product_search_score.mean()
mean_D=train_df[train_df['customer_active_segment']=='D'].customer_product_search_score.mean()
mean_None=train_df[train_df['customer_active_segment']=='None'].customer_product_search_score.mean()

train_df['customer_product_search_score'] = train_df[['customer_product_search_score','customer_active_segment']].apply(impute_missing_value,args=[mean_A,mean_AA,mean_B,mean_C,mean_D,mean_None],axis=1)
test_df['customer_product_search_score'] = test_df[['customer_product_search_score','customer_active_segment']].apply(impute_missing_value,args=[mean_A,mean_AA,mean_B,mean_C,mean_D,mean_None],axis=1)

mean_A=train_df[train_df['customer_active_segment']=='A'].customer_stay_score.mean()
mean_AA=train_df[train_df['customer_active_segment']=='AA'].customer_stay_score.mean()
mean_B=train_df[train_df['customer_active_segment']=='B'].customer_stay_score.mean()
mean_C=train_df[train_df['customer_active_segment']=='C'].customer_stay_score.mean()
mean_D=train_df[train_df['customer_active_segment']=='D'].customer_stay_score.mean()
mean_None=train_df[train_df['customer_active_segment']=='None'].customer_stay_score.mean()

train_df['customer_stay_score'] = train_df[['customer_stay_score','customer_active_segment']].apply(impute_missing_value,args=[mean_A,mean_AA,mean_B,mean_C,mean_D,mean_None],axis=1)
test_df['customer_stay_score'] = test_df[['customer_stay_score','customer_active_segment']].apply(impute_missing_value,args=[mean_A,mean_AA,mean_B,mean_C,mean_D,mean_None],axis=1)

mean_A=train_df[train_df['customer_active_segment']=='A'].customer_product_variation_score.mean()
mean_AA=train_df[train_df['customer_active_segment']=='AA'].customer_product_variation_score.mean()
mean_B=train_df[train_df['customer_active_segment']=='B'].customer_product_variation_score.mean()
mean_C=train_df[train_df['customer_active_segment']=='C'].customer_product_variation_score.mean()
mean_D=train_df[train_df['customer_active_segment']=='D'].customer_product_variation_score.mean()
mean_None=train_df[train_df['customer_active_segment']=='None'].customer_product_variation_score.mean()

train_df['customer_product_variation_score'] = train_df[['customer_product_variation_score','customer_active_segment']].apply(impute_missing_value,args=[mean_A,mean_AA,mean_B,mean_C,mean_D,mean_None],axis=1)
test_df['customer_product_variation_score'] = test_df[['customer_product_variation_score','customer_active_segment']].apply(impute_missing_value,args=[mean_A,mean_AA,mean_B,mean_C,mean_D,mean_None],axis=1)

mean_A=train_df[train_df['customer_active_segment']=='A'].customer_order_score.mean()
mean_AA=train_df[train_df['customer_active_segment']=='AA'].customer_order_score.mean()
mean_B=train_df[train_df['customer_active_segment']=='B'].customer_order_score.mean()
mean_C=train_df[train_df['customer_active_segment']=='C'].customer_order_score.mean()
mean_D=train_df[train_df['customer_active_segment']=='D'].customer_order_score.mean()
mean_None=train_df[train_df['customer_active_segment']=='None'].customer_order_score.mean()

train_df['customer_order_score'] = train_df[['customer_order_score','customer_active_segment']].apply(impute_missing_value,args=[mean_A,mean_AA,mean_B,mean_C,mean_D,mean_None],axis=1)
test_df['customer_order_score'] = test_df[['customer_order_score','customer_active_segment']].apply(impute_missing_value,args=[mean_A,mean_AA,mean_B,mean_C,mean_D,mean_None],axis=1)

print('For Training Datatset','\n')
print(train_df.isnull().sum(),'\n')
print('For Testing Datatset','\n')
print(test_df.isnull().sum())

"""###Dealing with categorical features"""

train_df=pd.get_dummies(train_df)
test_df=pd.get_dummies(test_df)

"""###Dropping the target feature"""

X=train_df.drop('customer_category',1)
y=train_df['customer_category']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=1)

"""###Model Building

#Logisitic Regression with default parameters
"""

model=LogisticRegression(C= 1, penalty= 'l2', solver= 'liblinear')
model.fit(X_train, y_train)
y_pred=model.predict(X_test)
print(metrics.precision_score(y_test , y_pred , average='macro'))
print(confusion_matrix(y_test,y_pred))
print(classification_report(y_test,y_pred))

"""#Decision Tree Classifier with default parameters"""

model = DecisionTreeClassifier()
model.fit(X_train, y_train)
y_pred=model.predict(X_test)
print(metrics.precision_score(y_test , y_pred , average='macro'))
print(confusion_matrix(y_test,y_pred))
print(classification_report(y_test,y_pred))

"""#Random Forest with default parameters"""

model = RandomForestClassifier(bootstrap=False,criterion='entropy', max_depth= 50,max_features='sqrt',
                                min_samples_leaf=2,min_samples_split=4,n_estimators= 100)
model.fit(X_train, y_train)
y_pred=model.predict(X_test)
print(metrics.precision_score(y_test , y_pred , average='macro'))
print(confusion_matrix(y_test,y_pred))
print(classification_report(y_test,y_pred))

"""#XGBClassifier with default parameters"""

model = XGBClassifier()#colsample_bylevel =0.5,colsample_bytree= 0.3,gamma= 2,
                      #learning_rate= 0.2,max_depth= 6,min_child_weight=3,subsample=0.3)
model.fit(X_train, y_train)
y_pred=model.predict(X_test)
print(metrics.precision_score(y_test , y_pred , average='macro'))
print("(tn, fp, fn, tp):",confusion_matrix(y_test,y_pred).ravel())
print(classification_report(y_test,y_pred))

"""#Support Vector Machine Classifier with default parameters"""

model = SVC()#C=0.1,decision_function_shape='ovo',degree= 2,gamma= 'scale',kernel= 'linear')
# {'C': 0.1,
#  'decision_function_shape': 'ovo',
#  'degree': 1,
#  'gamma': 'auto',
#  'kernel': 'poly'}
model.fit(X_train, y_train)
y_pred=model.predict(X_test)
print(metrics.precision_score(y_test , y_pred , average='macro'))
print("(tn, fp, fn, tp):",confusion_matrix(y_test,y_pred).ravel())
print(classification_report(y_test,y_pred))

"""#Logistic Regression Hyperparameter Tuning"""

param_grid = {
    'solver' : ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],
    'penalty' : ['none', 'l1', 'l2', 'elasticnet'],
    'C': [100, 10, 1,0.1, 0.01]
}
  
model = GridSearchCV(LogisticRegression(), param_grid, cv=3,refit = True, verbose = 3, scoring='precision_macro', n_jobs=4) 
# fitting the model for grid search 
model.fit(X_train, y_train) 
y_pred=model.predict(X_test)
print(metrics.precision_score(y_test , y_pred , average='macro'))
print("(tn, fp, fn, tp):",confusion_matrix(y_test,y_pred).ravel())
print(classification_report(y_test,y_pred))

model.best_params_

"""#Hyperparameter Tuning for Random Forest Classifier"""

param_grid = {
    'bootstrap': [False],
    'criterion': ['gini','entropy'],
    'max_depth': [30, 40, 50],
    'max_features': ['auto','sqrt','log2'],
    'min_samples_leaf': [1, 2],
    'min_samples_split': [2, 3, 4],
    'n_estimators': [100, 200, 300]
}
  
model = GridSearchCV(RandomForestClassifier(), param_grid, cv=3,refit = True, verbose = 3, scoring='precision_macro', n_jobs=3) 
# fitting the model for grid search 
model.fit(X_train, y_train) 
y_pred=model.predict(X_test)
print(metrics.precision_score(y_test , y_pred , average='macro'))
print("(tn, fp, fn, tp):",confusion_matrix(y_test,y_pred).ravel())
print(classification_report(y_test,y_pred))

model.best_params_

"""#Hyperparameter Tuning for XGB Classifier"""

model = XGBClassifier(colsample_bylevel =0.5,colsample_bytree= 0.3,gamma= 2,
                      learning_rate= 0.2,max_depth= 6,min_child_weight=3,subsample=0.3)
model.fit(X_train, y_train)
y_pred=model.predict(X_test)
print(metrics.precision_score(y_test , y_pred , average='macro'))
print("(tn, fp, fn, tp):",confusion_matrix(y_test,y_pred).ravel())
print(classification_report(y_test,y_pred))

param_grid = {
        'max_depth': [5, 6, 7],
        'min_child_weight': [1,2,3],
        'gamma': [0,1,2,3],
        'learning_rate': [0.2,0.3,0.4],
        'subsample': [0.3,0.5,0.1],
        'colsample_bylevel': [0.3,0.5,1],
        'colsample_bytree': [0.3,0.5,1],
}
  
model = GridSearchCV(XGBClassifier(), param_grid, cv=3,refit = True, verbose = 2,scoring='precision_macro',n_jobs=5)
model.fit(X_train, y_train)
y_pred=model.predict(X_test)
print(metrics.precision_score(y_test , y_pred , average='macro'))
print("(tn, fp, fn, tp):",confusion_matrix(y_test,y_pred).ravel())
print(classification_report(y_test,y_pred))

model.best_params_

"""#Hyperparameter Tuning for SVC Classifier"""

model = SVC(C= 0.1,decision_function_shape= 'ovo',degree=1,gamma= 'auto',kernel='poly')#)C=0.1,decision_function_shape='ovo',degree= 2,gamma= 'scale',kernel= 'linear')
model.fit(X_train, y_train)
y_pred=model.predict(X_test)
print(metrics.precision_score(y_test , y_pred , average='macro'))
print("(tn, fp, fn, tp):",confusion_matrix(y_test,y_pred).ravel())
print(classification_report(y_test,y_pred))

param_grid = {'C': [0.1,1,10],  
              'gamma': ['scale','auto'], 
              'kernel': ['poly'],
              'degree':[1,2,3],
              'decision_function_shape':['ovo','ovr']}  
  
model = GridSearchCV(SVC(), param_grid, cv=3,refit = True, verbose = 2,scoring='precision_macro')
model.fit(X_train, y_train)
y_pred=model.predict(X_test)
print(metrics.precision_score(y_test , y_pred , average='macro'))
print("(tn, fp, fn, tp):",confusion_matrix(y_test,y_pred).ravel())
print(classification_report(y_test,y_pred))

model.best_params_

"""#Comparison of Precision score, recall score and F1 score before and after hyperparameter tuning"""

Algorithms=['Logistic Regression','Decision Tree','Random Forest','XG Boost','SVC']
precision_before_tuning=[0.97, 0.90, 0.97, 0.97, 0.96]
recall_before_tuning=[0.8,0.82,0.85,0.86,0.63]
F1_before_tuning=[0.88, 0.83, 0.90,0.91, 0.77]

precision_score_before_tuning=pd.DataFrame(columns=['Precision_Score','Algorithm'])
precision_score_before_tuning['Algorithm']=Algorithms
precision_score_before_tuning['Precision_Score']=precision_before_tuning
precision_score_before_tuning.plot(kind='bar',x='Algorithm')

recall_score_before_tuning=pd.DataFrame(columns=['Recall','Algorithm'])
recall_score_before_tuning['Algorithm']=Algorithms
recall_score_before_tuning['Recall']=recall_before_tuning
recall_score_before_tuning.plot(kind='bar',x='Algorithm')

F1_score_before_tuning=pd.DataFrame(columns=['F1','Algorithm'])
F1_score_before_tuning['Algorithm']=Algorithms
F1_score_before_tuning['F1']=F1_before_tuning
F1_score_before_tuning.plot(kind='bar',x='Algorithm')

Algorithms=['Logistic Regression','Random Forest','XG Boost','SVC']
precision_after_tuning=[0.98, 0.96,0.96,0.97]
recall_after_tuning=[0.80,0.86,0.84,0.76]
F1_after_tuning=[0.88, 0.91,0.90,0.85]

precision_score_after_tuning=pd.DataFrame(columns=['Precision_Score','Algorithm'])
precision_score_after_tuning['Algorithm']=Algorithms
precision_score_after_tuning['Precision_Score']=precision_after_tuning
precision_score_after_tuning.plot(kind='bar',x='Algorithm')

recall_score_after_tuning=pd.DataFrame(columns=['Recall','Algorithm'])
recall_score_after_tuning['Algorithm']=Algorithms
recall_score_after_tuning['Recall']=recall_after_tuning
recall_score_after_tuning.plot(kind='bar',x='Algorithm')

F1_score_after_tuning=pd.DataFrame(columns=['F1','Algorithm'])
F1_score_after_tuning['Algorithm']=Algorithms
F1_score_after_tuning['F1']=F1_after_tuning
F1_score_after_tuning.plot(kind='bar',x='Algorithm')

"""#Results obtained from the hackathon portal for different algorithms"""

Algorithms=['Logistic Regression','Decision Tree','Random Forest','XG Boost','SVC']
Score_From_Hackathon_Portal=[92.83,80.69,92.8,92.63,96.4]
df=pd.DataFrame(columns=['Score','Algorithm'])
df['Algorithm']=Algorithms
df['Score']=Score_From_Hackathon_Portal
df.plot(kind='bar',x='Algorithm')

"""###Code snippet for calculating the predicted values and storing the results"""

finaldf=pd.DataFrame(columns=['customer_id','customer_category'])
finaldf['customer_id']=test_df_customer_id
print(finaldf.head())

y_pred = model.predict(test_df)
finaldf['customer_category']=y_pred
print(finaldf.head(),finaldf.shape)

finaldf.to_csv('Amazon_ML_Challenge_Prediction_43_Final_SVC.csv',index=False)

